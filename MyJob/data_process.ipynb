{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "import re\n",
    "from entity_recognize import kd_ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集、测试集、验证集数据读取\n",
    "\n",
    "feature2chinese = {\"Disease\": \"疾病\", \"Symptom\": \"症状\", \"Attribute\": \"属性\", \"Test\": \"检查\", \"Medicine\": \"药物\"}\n",
    "\n",
    "with open(\"../data/train/train.pk\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "    \n",
    "with open(\"../data/evalution/dev.pk\", \"rb\") as f:\n",
    "    dev_data = pickle.load(f)\n",
    "    \n",
    "with open(\"../data/test/test.pk\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将测试集和验证集的数据合并处理。主要用于预训练\n",
    "dev_test_data = dev_data + test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本生成任务训练数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将测试集和验证集中的历史对话数据用于文本生成训练任务\n",
    "dev_test_data_end_with_doctor = copy.deepcopy(dev_test_data)\n",
    "dev_test_data_for_generation = []\n",
    "for dtd in dev_test_data_end_with_doctor:\n",
    "    try:\n",
    "        while dtd['history'][-1][:2] == '患者':\n",
    "            dtd['history'].pop()\n",
    "        dev_test_data_for_generation.append(dtd)\n",
    "    except:\n",
    "        continue\n",
    "del dev_test_data_end_with_doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用于将同一句话中的不同特征进行整合\n",
    "def cat_feature_info(dt):\n",
    "    together_list = []\n",
    "    for feature in feature2chinese.keys():\n",
    "        together_list.extend(dt[feature])\n",
    "    return together_list\n",
    "\n",
    "def dev_type2train_type(input_list):\n",
    "    output_list = []\n",
    "    for text in input_list:\n",
    "        role, sent = text[:2], text[3:]\n",
    "        entity = list(kd_ana.convert_sen_to_entity_set(sent))\n",
    "        role = \"Patient\" if role == \"患者\" else \"Doctor\"\n",
    "        output_list.append({\"Attribute\": entity, 'Disease': [], 'Medicine': [], 'Sentence': sent, 'Symptom': [], 'Test': [], 'id': role})\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将用于文本生成的测试集数据和验证集数据转换成训练集的格式，从而统一处理\n",
    "dev_test_data_for_generation2train_type = []\n",
    "for dt in dev_test_data_for_generation:\n",
    "    dev_test_data_for_generation2train_type.append(dev_type2train_type(dt[\"history\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 将验证集和测试集中的数据合并到训练集中，共同用于训练文本生成任务。\n",
    "train_data += dev_test_data_for_generation2train_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将文本长度大于1020的对话，进行截断\n",
    "def cut_dialog_without_entity(line_info, max_lenght=1020):\n",
    "    inp_list = line_info[\"input\"]\n",
    "    cut_id = 0\n",
    "    cut_flag = False\n",
    "    while True:\n",
    "        cnt = 0\n",
    "        for inp in inp_list:\n",
    "            for _inp in inp:\n",
    "                cnt += len(_inp) + 1\n",
    "        if cnt <= max_lenght:\n",
    "            line_info[\"input\"] = inp_list\n",
    "            return cut_flag\n",
    "        ned_cut = cnt - max_lenght\n",
    "        if len(inp_list) >= 10:\n",
    "            if len(inp_list[0])==1:\n",
    "                inp_list = inp_list[1:]\n",
    "                line_info[\"begin_role\"] = 1 if line_info[\"begin_role\"]==0 else 0\n",
    "                line_info[\"output\"].pop(0)\n",
    "            else:\n",
    "                inp_list[0] = inp_list[0][1:]\n",
    "                line_info[\"output\"][0].pop(0)\n",
    "        else:\n",
    "            for i, sent in enumerate(inp_list[cut_id]):\n",
    "                to_cut = min(ned_cut, len(sent))\n",
    "                inp_list[cut_id][i] = sent[to_cut:]\n",
    "                ned_cut -= to_cut\n",
    "                if ned_cut <= 0:\n",
    "                    break\n",
    "            cut_id += 1\n",
    "        cut_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished forword:27\n",
      "finished forword:27\n",
      "finished forword:26\n",
      "finished forword:25\n",
      "finished forword:23\n",
      "finished forword:25\n",
      "finished forword:25\n",
      "finished forword:25\n",
      "finished forword:26\n",
      "finished forword:24\n",
      "finished forword:28\n",
      "finished forword:29\n",
      "finished forword:25\n",
      "finished forword:24\n",
      "finished forword:25\n",
      "finished forword:28\n",
      "finished forword:28\n",
      "finished forword:28\n",
      "finished forword:26\n",
      "finished forword:27\n",
      "finished forword:21\n"
     ]
    }
   ],
   "source": [
    "# 生成 “文本生成任务训练数据”\n",
    "# 策略：每一个dialog 有多轮对话。训练时，有两种策略：1.在同一个样本中对所有医生的话术进行teacher force learning; \n",
    "# 2.将同一个样本拆封成多个子样本，每个子样本只对一轮对话中的医生话术进行teacher force learning\n",
    "# 我们采用第二种策略\n",
    "train_num = len(train_data)   \n",
    "_train_data = []\n",
    "with open(\"../data/generation_train_final.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    for t in range(10):  # 生成多个epoch的训练数据，并且将每个epoch的数据打散\n",
    "        random.shuffle(train_data)   # 在训练数据中手动加入训练样本shuffle\n",
    "        for r,dt in enumerate(train_data):\n",
    "            _train_data.append(dt)       # 每1000个样本，构成一个轮回。主要是防止训练时，是先把单轮对话数据训练完后再训练两轮，依次类推；\n",
    "                                        # 如果这样，容易使模型陷入局部解\n",
    "            if len(_train_data) == 1000 or r == train_num - 1:\n",
    "                for i in range(1, 30):   # 最多支持30轮对话\n",
    "                    break_forword = False\n",
    "                    for _dt in _train_data:\n",
    "                        line_info = {\"begin_role\": 0, \"input\": [], \"output\": []}\n",
    "                        if _dt[0][\"id\"] == \"Doctor\":\n",
    "                            line_info[\"begin_role\"] = 1\n",
    "                        patient_in_flag = False\n",
    "                        last_id = \"\"\n",
    "                        cnt = 0\n",
    "                        for seg in _dt:\n",
    "                            feature_info = cat_feature_info(seg)\n",
    "                            dialo_sentence = seg['Sentence']\n",
    "#                             dialo_sentence = whole_word_process(seg['Sentence'], feature_info)\n",
    "                            if seg[\"id\"] == \"Doctor\":\n",
    "                                cnt += 1\n",
    "                                if last_id == \"Doctor\":\n",
    "                                    line_info[\"input\"][-1].append(dialo_sentence)\n",
    "                                    line_info[\"output\"][-1].append(feature_info)\n",
    "                                else:\n",
    "                                    line_info[\"input\"].append([dialo_sentence])\n",
    "                                    line_info[\"output\"].append([feature_info])\n",
    "                                if cnt == i and line_info[\"begin_role\"] == 0:\n",
    "                                    break_forword =True\n",
    "                                    flag = cut_dialog_without_entity(line_info)\n",
    "#                                     if flag:\n",
    "                                    f.write(json.dumps(line_info, ensure_ascii=False) + '\\n')\n",
    "                                    break\n",
    "                                elif cnt > i and line_info[\"begin_role\"] == 1 and patient_in_flag:\n",
    "                                    break_forword = True\n",
    "                                    flag = cut_dialog_without_entity(line_info)\n",
    "#                                     if flag:\n",
    "                                    f.write(json.dumps(line_info, ensure_ascii=False) + '\\n')\n",
    "                                    break\n",
    "                                last_id = \"Doctor\"\n",
    "                            elif seg[\"id\"] == \"Patient\":\n",
    "                                patient_in_flag = True\n",
    "                                if last_id == \"Patient\":\n",
    "                                    line_info[\"input\"][-1].append(dialo_sentence)\n",
    "                                    line_info[\"output\"][-1].append(feature_info)\n",
    "                                else:\n",
    "                                    line_info[\"input\"].append([dialo_sentence])\n",
    "                                    line_info[\"output\"].append([feature_info])\n",
    "                                last_id = \"Patient\"\n",
    "\n",
    "                    if not break_forword:\n",
    "                        print(\"finished forword:{}\".format(i))\n",
    "                        break\n",
    "                _train_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 用于entity train data生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将entity_list 中的entity作为单个char字符，进行训练\n",
    "def whole_word_process_without_entity(sentence: str, feature_list: List) -> str:\n",
    "    if not feature_list:\n",
    "        return sentence\n",
    "    rst = list(sentence)\n",
    "    add_n = 0\n",
    "    for node in re.finditer(\"|\".join(feature_list), sentence):\n",
    "        beg, end = node.span()\n",
    "        rst.insert(beg+add_n, \"##\")  # 在entity名词前后分别加上 ‘##’ 、‘$’。在tokenizer.encode时进行识别并标为单token\n",
    "        add_n += 1\n",
    "        rst.insert(end+add_n, \"$\")\n",
    "        add_n += 1\n",
    "    return \"\".join(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22208\n"
     ]
    }
   ],
   "source": [
    "#char级别\n",
    "train_num = len(train_data)   \n",
    "print(train_num)\n",
    "_train_data = []\n",
    "with open(\"../data/train_total_dialog_for_entity_char.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    for t in range(15):\n",
    "        random.shuffle(train_data)   # 在训练数据中手动加入训练样本shuffle\n",
    "        for _dt in train_data:\n",
    "            line_info = {\"begin_role\": 0, \"input\": [], \"output\": []}\n",
    "            if _dt[0][\"id\"] == \"Doctor\":\n",
    "                line_info[\"begin_role\"] = 1\n",
    "            last_id = \"\"\n",
    "            for seg in _dt:\n",
    "                feature_info = cat_feature_info(seg)\n",
    "                dialo_sentence = whole_word_process_without_entity(seg['Sentence'], feature_info)\n",
    "                if seg[\"id\"] == \"Doctor\":\n",
    "                    if last_id == \"Doctor\":\n",
    "                        line_info[\"input\"][-1].append(dialo_sentence)\n",
    "                        line_info[\"output\"][-1].append(feature_info)\n",
    "                    else:\n",
    "                        line_info[\"input\"].append([dialo_sentence])\n",
    "                        line_info[\"output\"].append([feature_info])                    \n",
    "                    last_id = \"Doctor\"\n",
    "                elif seg[\"id\"] == \"Patient\":\n",
    "                    patient_in_flag = True\n",
    "                    if last_id == \"Patient\":\n",
    "                        line_info[\"input\"][-1].append(dialo_sentence)\n",
    "                        line_info[\"output\"][-1].append(feature_info)\n",
    "                    else:\n",
    "                        line_info[\"input\"].append([dialo_sentence])\n",
    "                        line_info[\"output\"].append([feature_info])\n",
    "                    last_id = \"Patient\"\n",
    "            flag = cut_dialog_without_entity(line_info)\n",
    "           # print(line_info)\n",
    "            f.write(json.dumps(line_info, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 评测数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r\"E:\\BaiduNetdiskDownload\\ccks21_mdg_evaluation\\test_sample.pk\", \"rb\") as f:\n",
    "    dt = pickle.load(f)\n",
    "    \n",
    "with open(r\"E:\\BaiduNetdiskDownload\\ccks21_mdg_evaluation\\test_sample_reference.pk\", \"rb\") as f:\n",
    "    dt_response = pickle.load(f)\n",
    "    \n",
    "feature_list = []\n",
    "for line in open(\"../data/entity_list.txt\", \"r\", encoding=\"utf8\"):\n",
    "    feature_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def whole_word_process_without_entity(sentence: str, feature_list: List) -> str:\n",
    "    if not feature_list:\n",
    "        return sentence\n",
    "    rst = list(sentence)\n",
    "    add_n = 0\n",
    "    for node in re.finditer(\"|\".join(feature_list), sentence):\n",
    "        beg, end = node.span()\n",
    "        rst.insert(beg+add_n, \"##\")\n",
    "        add_n += 1\n",
    "        rst.insert(end+add_n, \"$\")\n",
    "        add_n += 1\n",
    "    return \"\".join(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/evalution_data.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for dialog, response in zip(dt, dt_response):\n",
    "        normalization_data = {\"begin_role\": 0, \"input\":[], \"output\":[]}\n",
    "        response = whole_word_process_without_entity(response, feature_list)\n",
    "        response_entity = list(kd_ana.convert_sen_to_entity_set(response))\n",
    "        last_role = \"\"\n",
    "        dialog = dialog[\"history\"]\n",
    "        if dialog[0].startswith(\"医生\"):\n",
    "            normalization_data[\"begin_role\"] = 1 \n",
    "#             last_role = \"doctor\"\n",
    "        for _dialog in dialog:\n",
    "            role, text = _dialog[:2], _dialog[3:]\n",
    "            text = whole_word_process_without_entity(text, feature_list)\n",
    "            entity = list(kd_ana.convert_sen_to_entity_set(text))\n",
    "            if role != last_role:\n",
    "                normalization_data[\"input\"].append([text])\n",
    "                normalization_data[\"output\"].append([entity])\n",
    "            else:\n",
    "                normalization_data[\"input\"][-1].append(text)\n",
    "                normalization_data[\"output\"][-1].append(entity)\n",
    "            last_role = role\n",
    "        if last_role == \"患者\":\n",
    "            normalization_data[\"input\"].append([response])\n",
    "            normalization_data[\"output\"].append([response_entity])\n",
    "        else:\n",
    "            normalization_data[\"input\"][-1].append(response)\n",
    "            normalization_data[\"output\"][-1].append(response_entity)\n",
    "        f.write(json.dumps(normalization_data, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation过程中，采用word level的roformer模型，训练集中新增的部分word没有在vocab当中，需要筛选出并手动加上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\LASTFI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.631 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.load_userdict(\"../data/entity_list.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_list = [line.strip() for line in open(\"../data/entity_list.txt\", \"r\", encoding=\"utf8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature2chinese = {\"Disease\": \"疾病\", \"Symptom\": \"症状\", \"Attribute\": \"属性\", \"Test\": \"检查\", \"Medicine\": \"药物\"}\n",
    "\n",
    "with open(r\"E:\\BaiduNetdiskDownload\\ccks21_mdg_dataset\\train.pk\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_cnt_dict = {}\n",
    "for td in train_data:\n",
    "    for _td in td:\n",
    "        for part in jieba.cut(_td[\"Sentence\"]):\n",
    "            if len(part) > 1:\n",
    "                word_cnt_dict[part] = word_cnt_dict.get(part, 0) + 1\n",
    "                \n",
    "word_cont_dict_filter = {key:val for key, val in word_cnt_dict.items() if val >=15}  # 如果加入预训练，可以适当减小过滤阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenier_already_haven_word = [line.strip() for line in \n",
    "                               open(r\"E:\\BaiduNetdiskDownload\\chinese_roformer_L-12_H-768_A-12\\vocab.txt\", \"r\", encoding=\"utf8\")]\n",
    "\n",
    "# roformer_add_whole_word_except_entity.txt 属于当前数据集的新增词，且不包含entity_list中的词\n",
    "with open(\"../data/roformer_add_whole_word_except_entity.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for key in word_cont_dict_filter.keys():\n",
    "        if key not in entity_list:\n",
    "            if key in tokenier_already_haven_word:\n",
    "                f.write(key + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wobert vocab number: 50000\n",
      "jieba add_word number: 2362\n"
     ]
    }
   ],
   "source": [
    "jieba_add_word = []\n",
    "\n",
    "tokenier_already_haven_word = [line.strip() for line in open(r\"E:\\BaiduNetdiskDownload\\chinese_roformer_L-12_H-768_A-12\\vocab.txt\", \"r\", encoding=\"utf8\")]\n",
    "print(\"wobert vocab number:\", len(tokenier_already_haven_word))\n",
    "\n",
    "for line in open(\"../data/entity_list.txt\",\"r\", encoding=\"utf8\"):\n",
    "    word = line.strip()\n",
    "    if word not in tokenier_already_haven_word:\n",
    "        jieba_add_word.append(word)\n",
    "    \n",
    "for key in word_cont_dict_filter:\n",
    "    if key not in tokenier_already_haven_word:  # tokenier中只是加入了20000个高频的词\n",
    "        jieba_add_word.append(key)\n",
    "print(\"jieba add_word number:\", len(set(jieba_add_word)))\n",
    "with open(\"../data/jieba_add.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for word in set(jieba_add_word):\n",
    "        f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
